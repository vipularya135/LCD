{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1ace24e8-c4e9-4dbe-a5b8-89584ceecfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import math\n",
    "\n",
    "# Ensure reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Check if GPU is available and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data preprocessing\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def prepare_data(dataset_name):\n",
    "    print('==> Preparing data..')\n",
    "    \n",
    "    if dataset_name == 'cifar10':\n",
    "        # CIFAR-10 Normalization\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "        ])\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "        ])\n",
    "        train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "        test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "    \n",
    "    elif dataset_name == 'cifar100':\n",
    "        # CIFAR-100 Normalization\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "        ])\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "        ])\n",
    "        train_set = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "        test_set = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "    \n",
    "    elif dataset_name == 'svhn':\n",
    "        # SVHN Normalization\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4377, 0.4438, 0.4728), (0.1980, 0.2010, 0.1970)),\n",
    "        ])\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4377, 0.4438, 0.4728), (0.1980, 0.2010, 0.1970)),\n",
    "        ])\n",
    "        train_set = datasets.SVHN(root='./data', split='train', download=True, transform=transform_train)\n",
    "        test_set = datasets.SVHN(root='./data', split='test', download=True, transform=transform_test)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset name. Choose from 'cifar10', 'cifar100', or 'svhn'.\")\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    initial_train_set, remainder = torch.utils.data.random_split(train_set, [10000, len(train_set) - 10000])\n",
    "\n",
    "    print(f\"Size of train_set: {len(train_set)}\")\n",
    "    print(f\"Size of test_set: {len(test_set)}\")\n",
    "\n",
    "    return initial_train_set, remainder, test_set\n",
    "\n",
    "\n",
    "        \n",
    "def train_model(model, train_loader, epochs, learning_rate):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "     \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}')\n",
    "        \n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "    \n",
    "\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "def calculate_cluster_centers(embeddings, num_clusters):\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=0, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    return cluster_centers\n",
    "\n",
    "def get_most_diverse_samples(tsne_results, cluster_centers, num_diverse_samples):\n",
    "    distances = euclidean_distances(tsne_results, cluster_centers)\n",
    "    min_distances = np.max(distances, axis=1)\n",
    "    sorted_indices = np.argsort(min_distances)\n",
    "    diverse_indices = sorted_indices[:num_diverse_samples]\n",
    "    return diverse_indices\n",
    "\n",
    "def extract_embeddings(model, test):\n",
    "    test_loader = data.DataLoader(test, batch_size=64, shuffle=False)\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    targets_list = []\n",
    "    with torch.no_grad():\n",
    "        for images, targets in test_loader:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            intermediate_features = model(images)\n",
    "            embeddings.extend(intermediate_features.view(intermediate_features.size(0), -1).tolist())\n",
    "            targets_list.append(targets)\n",
    "    return embeddings\n",
    "\n",
    "def least_confidence_images(model, test_dataset, k=2500):\n",
    "    test_loader = data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    confidences = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, targets in test_loader:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            max_probs, _ = torch.max(probs, dim=1)\n",
    "            confidences.extend(max_probs.cpu().tolist())\n",
    "            labels.extend(targets.cpu().tolist())\n",
    "    confidences = torch.tensor(confidences)\n",
    "    k = min(k, len(confidences))\n",
    "    _, indices = torch.topk(confidences, k, largest=False)\n",
    "    return data.Subset(test_dataset, indices), [labels[i] for i in indices]\n",
    "\n",
    "def high_confidence_images(model, test_dataset, k=2500):\n",
    "    test_loader = data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    confidences = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, targets in test_loader:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            max_probs, _ = torch.max(probs, dim=1)\n",
    "            confidences.extend(max_probs.cpu().tolist())\n",
    "            labels.extend(targets.cpu().tolist())\n",
    "    confidences = torch.tensor(confidences)\n",
    "    k = min(k, len(confidences))\n",
    "    _, indices = torch.topk(confidences, k, largest=True)\n",
    "    return data.Subset(test_dataset, indices), [labels[i] for i in indices]\n",
    "    \n",
    "def HC_diverse(embed_model, remainder, n=None):\n",
    "    high_conf_images, high_conf_indices = high_confidence_images(embed_model, remainder, k=len(remainder) if 2*n > len(remainder) else 2*n)\n",
    "    high_conf_embeddings = extract_embeddings(embed_model, high_conf_images)\n",
    "    high_conf_embeddings = np.array([np.array(e) for e in high_conf_embeddings])\n",
    "    tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n",
    "    tsne_results = tsne.fit_transform(high_conf_embeddings)\n",
    "    cluster_centers = calculate_cluster_centers(tsne_results, 10)\n",
    "    diverse_indices = get_most_diverse_samples(tsne_results, cluster_centers, n)\n",
    "    diverse_images = data.Subset(high_conf_images, diverse_indices)\n",
    "    return diverse_images, [high_conf_indices[i] for i in diverse_indices]\n",
    "\n",
    "def LC_diverse(embed_model, remainder, n=None):\n",
    "    least_conf_images, least_conf_indices = least_confidence_images(embed_model, remainder, k=len(remainder) if 2*n > len(remainder) else 2*n)\n",
    "    least_conf_embeddings = extract_embeddings(embed_model, least_conf_images)\n",
    "    least_conf_embeddings = np.array([np.array(e) for e in least_conf_embeddings])\n",
    "    tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n",
    "    tsne_results = tsne.fit_transform(least_conf_embeddings)\n",
    "    cluster_centers = calculate_cluster_centers(tsne_results, 10)\n",
    "    diverse_indices = get_most_diverse_samples(tsne_results, cluster_centers, n)\n",
    "    diverse_images = data.Subset(least_conf_images, diverse_indices)\n",
    "    return diverse_images, [least_conf_indices[i] for i in diverse_indices]\n",
    "\n",
    "def LC_HC(model, remainder, n=5000):\n",
    "    least_confident_2500, least_confident_indices = least_confidence_images(model, remainder, k=2500)\n",
    "    most_confident_2500, most_confident_indices = high_confidence_images(model, remainder, k=2500)\n",
    "    combined_dataset = data.ConcatDataset([least_confident_2500, most_confident_2500])\n",
    "    combined_indices = least_confident_indices + most_confident_indices\n",
    "    return combined_dataset, combined_indices\n",
    "\n",
    "def LC_HC_diverse(embed_model, remainder, n, low_conf_ratio=0.5, high_conf_ratio=0.5):\n",
    "    assert low_conf_ratio + high_conf_ratio == 1.0, \"The sum of low_conf_ratio and high_conf_ratio must be 1.0\"\n",
    "\n",
    "    n_low = int(n * low_conf_ratio)\n",
    "    n_high = int(n * high_conf_ratio)\n",
    "\n",
    "    least_conf_images, least_conf_indices = least_confidence_images(embed_model, remainder, k=len(remainder) if 2*n_low > len(remainder) else 2*n_low)\n",
    "    least_conf_embeddings = extract_embeddings(embed_model, least_conf_images)\n",
    "    least_conf_embeddings = np.array([np.array(e) for e in least_conf_embeddings])\n",
    "    tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n",
    "    tsne_results = tsne.fit_transform(least_conf_embeddings)\n",
    "    cluster_centers = calculate_cluster_centers(tsne_results, 10)\n",
    "    diverse_low_indices = get_most_diverse_samples(tsne_results, cluster_centers, n_low)\n",
    "    diverse_least_conf_images = data.Subset(least_conf_images, diverse_low_indices)\n",
    "\n",
    "    high_conf_images, high_conf_indices = high_confidence_images(embed_model, remainder, k=len(remainder) if 2*n_high > len(remainder) else 2*n_high)\n",
    "    high_conf_embeddings = extract_embeddings(embed_model, high_conf_images)\n",
    "    high_conf_embeddings = np.array([np.array(e) for e in high_conf_embeddings])\n",
    "    tsne_results = tsne.fit_transform(high_conf_embeddings)\n",
    "    cluster_centers = calculate_cluster_centers(tsne_results, 10)\n",
    "    diverse_high_indices = get_most_diverse_samples(tsne_results, cluster_centers, n_high)\n",
    "    diverse_high_conf_images = data.Subset(high_conf_images, diverse_high_indices)\n",
    "\n",
    "    combined_dataset = data.ConcatDataset([diverse_least_conf_images, diverse_high_conf_images])\n",
    "    combined_indices = [least_conf_indices[i] for i in diverse_low_indices] + [high_conf_indices[i] for i in diverse_high_indices]\n",
    "\n",
    "    return combined_dataset, combined_indices\n",
    "\n",
    "\n",
    "def train_until_empty(model, initial_train_set, remainder_set, test_set, max_iterations=15, batch_size=64, learning_rate=0.01, method=1):\n",
    "    exp_acc = []\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        print(f\"Starting Iteration {iteration}\")\n",
    "        print(f\"Remaindersize:{len(remainder_set)}\")\n",
    "        if len(remainder_set) == 0:\n",
    "            print(\"Dataset is empty. Stopping training.\")\n",
    "            break\n",
    "            \n",
    "        if method == 1:\n",
    "            train_data,used_indices = LC_HC(model, remainder_set, n=5000)\n",
    "        elif method == 2:\n",
    "            train_data,used_indices = LC_HC_diverse(model, remainder_set, n=5000)\n",
    "        elif method == 3:\n",
    "            train_data,used_indices = HC_diverse(model, remainder_set, n=5000)\n",
    "        elif method == 4:\n",
    "            train_data,used_indices = LC_diverse(model, remainder_set, n=5000)\n",
    "        else:\n",
    "            print(\"Invalid Method\")\n",
    "            return exp_acc\n",
    "    \n",
    "        initial_train_set = data.ConcatDataset([initial_train_set, train_data])\n",
    "        remainder_set = data.Subset(remainder_set, list(range(len(train_data), len(remainder_set))))\n",
    "            \n",
    "        print(f\"\\nTraining iteration {iteration + 1}\")\n",
    "        print(f\"Train Set Size: {len(initial_train_set)}, Remainder Size: {len(remainder_set)}\")\n",
    "        train_loader = data.DataLoader(initial_train_set, batch_size=batch_size, shuffle=True)\n",
    "        train_model(model, train_loader, epochs=50, learning_rate=learning_rate)\n",
    "\n",
    "        test_loader = data.DataLoader(test_set, batch_size=batch_size)\n",
    "        accuracy = test_model(model, test_loader)\n",
    "        exp_acc.append(accuracy)\n",
    "\n",
    "        print(f\"Iteration {iteration + 1}: Test Accuracy - {accuracy}\")\n",
    "\n",
    "    return exp_acc\n",
    "    \n",
    "import copy\n",
    "\n",
    "def run_all_methods(model, initial_train_set, remainder, test_set):\n",
    "    methods = [1, 2, 3, 4]\n",
    "    results = {}\n",
    "\n",
    "    for method in methods:\n",
    "        print(f\"\\nStarting training with method {method}\")\n",
    "        #model.load_state_dict(initial_model_state)\n",
    "        initial_train_set_copy = copy.deepcopy(initial_train_set)\n",
    "        remainder_copy = copy.deepcopy(remainder)\n",
    "        train_loader = data.DataLoader(initial_train_set_copy, batch_size=64, shuffle=True)\n",
    "        # train_model(model, train_loader, epochs=1, learning_rate=0.01)\n",
    "        test_loader = data.DataLoader(test_set, batch_size=64)\n",
    "        # test_model(model, test_loader)\n",
    "        exp_acc = train_until_empty(model, initial_train_set_copy, remainder_copy, test_set, \n",
    "                                    max_iterations=15, batch_size=64, learning_rate=0.01, method=method)\n",
    "        results[f\"method_{method}\"] = exp_acc\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b7d82212-0fc9-4f1d-ad15-83a8a4ba28a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "num_classes = 100\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_planes, growth_rate):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, 4*growth_rate, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(4*growth_rate)\n",
    "        self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = torch.cat([out,x], 1)\n",
    "        return out\n",
    "\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(in_planes)\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(F.relu(self.bn(x)))\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=10):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.growth_rate = growth_rate\n",
    "\n",
    "        num_planes = 2*growth_rate\n",
    "        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n",
    "        num_planes += nblocks[0]*growth_rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans1 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n",
    "        num_planes += nblocks[1]*growth_rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans2 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n",
    "        num_planes += nblocks[2]*growth_rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans3 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n",
    "        num_planes += nblocks[3]*growth_rate\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(num_planes)\n",
    "        self.linear = nn.Linear(num_planes, 100)\n",
    "\n",
    "    def _make_dense_layers(self, block, in_planes, nblock):\n",
    "        layers = []\n",
    "        for i in range(nblock):\n",
    "            layers.append(block(in_planes, self.growth_rate))\n",
    "            in_planes += self.growth_rate\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.trans3(self.dense3(out))\n",
    "        out = self.dense4(out)\n",
    "        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    '''Depthwise conv + Pointwise conv'''\n",
    "    def __init__(self, in_planes, out_planes, stride=1):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=in_planes, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv2 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        return out\n",
    "\n",
    "class MobileNet(nn.Module):\n",
    "    # (128,2) means conv planes=128, conv stride=2, by default conv stride=1\n",
    "    cfg = [64, (128,2), 128, (256,2), 256, (512,2), 512, 512, 512, 512, 512, (1024,2), 1024]\n",
    "\n",
    "    def __init__(self, num_classes=10):  # Change the number of classes to 10 for SVHN\n",
    "        super(MobileNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.layers = self._make_layers(in_planes=32)\n",
    "        self.linear = nn.Linear(1024, 100)\n",
    "\n",
    "    def _make_layers(self, in_planes):\n",
    "        layers = []\n",
    "        for x in self.cfg:\n",
    "            out_planes = x if isinstance(x, int) else x[0]\n",
    "            stride = 1 if isinstance(x, int) else x[1]\n",
    "            layers.append(Block(in_planes, out_planes, stride))\n",
    "            in_planes = out_planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layers(out)\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512 * block.expansion, 100)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "        \n",
    "        \n",
    "class Bottleneck2(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=True)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_planes,\n",
    "                    self.expansion * planes,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=True,\n",
    "                ),\n",
    "                nn.BatchNorm2d(self.expansion * planes),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "cfg = {\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.features = self._make_layers(cfg['VGG16'])\n",
    "        self.classifier = nn.Linear(512, 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "\"\"\"vgg16 = VGG16().to(device)\n",
    "resnet50 = ResNet(Bottleneck2, [3, 4, 6, 3], num_classes=10).to(device)\n",
    "resnet56 = ResNet(BasicBlock, [9, 9, 9, 9], num_classes=10).to(device)\n",
    "resnet18 = ResNet(BasicBlock, [2, 2, 2, 2], num_classes=10).to(device)\n",
    "mobilenet = MobileNet().to(device)\n",
    "densenet121 = DenseNet(Bottleneck, [6,12,24,16], growth_rate=32).to(device)\"\"\"\n",
    "\n",
    "all_models = {\n",
    "    'resnet18': ResNet(BasicBlock, [2, 2, 2, 2], num_classes=10).to(device),\n",
    "    'resnet50': ResNet(Bottleneck2, [3, 4, 6, 3], num_classes=10).to(device),\n",
    "    'resnet56': ResNet(BasicBlock, [9, 9, 9, 9], num_classes=10).to(device),\n",
    "    'mobilenet': MobileNet(num_classes=10).to(device),\n",
    "    'densenet121': DenseNet(Bottleneck, [6, 12, 24, 16], growth_rate=32, num_classes=10).to(device),\n",
    "    'vgg16': VGG16(num_classes=10).to(device)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57c7193-0ff2-4590-ac1c-e88be577e43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import copy\n",
    "import torch.utils.data as data\n",
    "\n",
    "# Function to check if GPU is available and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def save_model(model, model_name, dataset_name, stage):\n",
    "    # Create a directory to save the models if it doesn't exist\n",
    "    if not os.path.exists('saved_models'):\n",
    "        os.makedirs('saved_models')\n",
    "    \n",
    "    # Save the model state with the appropriate naming convention\n",
    "    model_path = f'saved_models/{model_name}_{dataset_name}_{stage}.pth'\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved: {model_path}\")\n",
    "\n",
    "def main():\n",
    "    datasets_list = ['cifar100', 'cifar10', 'svhn']\n",
    "\n",
    "    for dataset_name in datasets_list:\n",
    "        # Prepare data\n",
    "        initial_train_set, remainder_set, test_set = prepare_data(dataset_name)\n",
    "        test_loader = data.DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "\n",
    "        # Iterate over models\n",
    "        for model_name, model in all_models.items():\n",
    "            print(f\"\\nTraining {model_name} on {dataset_name}\")\n",
    "\n",
    "            # Move the model to the device (GPU or CPU)\n",
    "            model = model.to(device)\n",
    "            \n",
    "            # Train the model on the initial 10k images\n",
    "            train_loader = data.DataLoader(initial_train_set, batch_size=64, shuffle=True)\n",
    "\n",
    "            # Training loop\n",
    "            train_model(model, train_loader, epochs=50, learning_rate=0.01)\n",
    "\n",
    "            # Evaluate the model on the test set\n",
    "            accuracy = test_model(model, test_loader)\n",
    "            print(f\"Initial accuracy of {model_name} on {dataset_name}: {accuracy:.2f}%\")\n",
    "            \n",
    "            # Save the initial trained model after training on 10k images\n",
    "            save_model(model, model_name, dataset_name, 'initial')\n",
    "\n",
    "            # Iterate through each active learning method\n",
    "            methods = [1, 2, 3, 4]  # Define all the method numbers\n",
    "            for method in methods:\n",
    "                print(f\"\\nApplying method {method} on {model_name} for {dataset_name}\")\n",
    "\n",
    "                # Load the initial trained model for a fresh start\n",
    "                model_copy = copy.deepcopy(model)\n",
    "                model_copy.load_state_dict(torch.load(f'saved_models/{model_name}_{dataset_name}_initial.pth'))\n",
    "\n",
    "                # Move the copied model to the device\n",
    "                model_copy = model_copy.to(device)\n",
    "\n",
    "                # Deep copy of the data to prevent contamination between methods\n",
    "                initial_train_set_copy = copy.deepcopy(initial_train_set)\n",
    "                remainder_copy = copy.deepcopy(remainder_set)\n",
    "\n",
    "                # Run the selected method and save results\n",
    "                results = train_until_empty(model_copy, initial_train_set_copy, remainder_copy, test_set, \n",
    "                                            max_iterations=15, batch_size=64, learning_rate=0.01, method=method)\n",
    "                \n",
    "                # Save the retrained model for this method\n",
    "                save_model(model_copy, model_name, dataset_name, f'method_{method}')\n",
    "                \n",
    "                # Save or print the results\n",
    "                print(f\"Results for {model_name} on {dataset_name} with method {method}: {results}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
